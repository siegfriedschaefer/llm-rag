from typing import Any, Dict, List

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from langchain.schema.messages import BaseMessage

"""
This file contains the StreamingCustomHandler class which is used to handle the streaming of responses from the LLM model.
The StreamingCustomHandler class is a subclass of the BaseCallbackHandler class from the langchain.callbacks.base module.
The StreamingCustomHandler class has the following methods:
    - on_llm_new_token: This method is called when a new token is generated by the LLM model.
    - on_llm_start: This method is called when the LLM model starts running.
    - on_llm_end: This method is called when the LLM model ends running.
    - on_chat_model_start: This method is called when the chat model starts running.
    - on_chat_model_end: This method is called when the chat model ends running.
"""

class LLMStreamingHandler(BaseCallbackHandler):

    def __init__(self, queue) -> None:
        super().__init__()
        self._queue = queue
        self._stop_signal = None

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self._queue.put(token)

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when LLM starts running."""
        print("generation started")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when LLM ends running."""
        print("\n\ngeneration concluded")
        self._queue.put(self._stop_signal)

    def on_chat_model_start(
        self,
        serialized: Dict[str, Any],
        messages: List[List[BaseMessage]],
        **kwargs: Any
    ) -> None:
        """Run when chatmodel starts running."""
        print("chat model started ")

